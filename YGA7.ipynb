{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e40b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33a5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/home/ubuntu/Suraj61/text_sample')\n",
    "raw_text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c7ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "726c2128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Despite', 'the', 'fact', 'that', 'piranhas', 'are', 'relatively', 'harmless', ',', 'many', 'people', 'continue', 'to', 'believe', 'the', 'pervasive', 'myth', 'that', 'piranhas', 'are'] \n",
      "\n",
      "Total tokens :  121\n"
     ]
    }
   ],
   "source": [
    "token_list = nltk.word_tokenize(raw_text)\n",
    "print(token_list[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d1abe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['despite', 'the', 'fact', 'that', 'piranhas', 'are', 'relatively', 'harmless', ',', 'many', 'people', 'continue', 'to', 'believe', 'the', 'pervasive', 'myth', 'that', 'piranhas', 'are'] \n",
      "\n",
      "Total tokens :  121\n"
     ]
    }
   ],
   "source": [
    "token_list1 = [word.lower() for word in token_list]\n",
    "print(token_list1[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31872175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['despite', 'the', 'fact', 'that', 'piranhas', 'are', 'relatively', 'harmless', 'many', 'people', 'continue', 'to', 'believe', 'the', 'pervasive', 'myth', 'that', 'piranhas', 'are', 'dangerous'] \n",
      "\n",
      "Total tokens :  111\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import punkt\n",
    "token_list2 = list(filter(lambda token : punkt.PunktToken(token).is_non_punct,token_list1))\n",
    "print(token_list2[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a88d24f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3e48e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd7169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('despite', 'IN')]\n",
      "[]\n",
      "[('fact', 'NN')]\n",
      "[]\n",
      "[('piranhas', 'NN')]\n",
      "[]\n",
      "[('relatively', 'RB')]\n",
      "[('harmless', 'NN')]\n",
      "[('many', 'JJ')]\n",
      "[('people', 'NNS')]\n",
      "[('continue', 'NN')]\n",
      "[]\n",
      "[('believe', 'VB')]\n",
      "[]\n",
      "[('pervasive', 'NN')]\n",
      "[('myth', 'NN')]\n",
      "[]\n",
      "[('piranhas', 'NN')]\n",
      "[]\n",
      "[('dangerous', 'JJ')]\n",
      "[]\n",
      "[('humans', 'NNS')]\n",
      "[]\n",
      "[('impression', 'NN')]\n",
      "[]\n",
      "[('piranhas', 'NN')]\n",
      "[]\n",
      "[('exacerbated', 'VBN')]\n",
      "[]\n",
      "[]\n",
      "[('mischaracterization', 'NN')]\n",
      "[]\n",
      "[('popular', 'JJ')]\n",
      "[('media', 'NNS')]\n",
      "[]\n",
      "[('example', 'NN')]\n",
      "[]\n",
      "[('promotional', 'JJ')]\n",
      "[('poster', 'NN')]\n",
      "[]\n",
      "[]\n",
      "[('1978', 'CD')]\n",
      "[('horror', 'NN')]\n",
      "[('film', 'NN')]\n",
      "[('piranha', 'NN')]\n",
      "[('features', 'NNS')]\n",
      "[]\n",
      "[('oversized', 'VBN')]\n",
      "[('piranha', 'NN')]\n",
      "[('poised', 'VBN')]\n",
      "[]\n",
      "[('bite', 'NN')]\n",
      "[]\n",
      "[('leg', 'NN')]\n",
      "[]\n",
      "[]\n",
      "[('unsuspecting', 'VBG')]\n",
      "[('woman', 'NN')]\n",
      "[]\n",
      "[]\n",
      "[('terrifying', 'VBG')]\n",
      "[('representation', 'NN')]\n",
      "[('easily', 'RB')]\n",
      "[('captures', 'NNS')]\n",
      "[]\n",
      "[('imagination', 'NN')]\n",
      "[]\n",
      "[('promotes', 'NNS')]\n",
      "[('unnecessary', 'JJ')]\n",
      "[('fear', 'NN')]\n",
      "[]\n",
      "[]\n",
      "[('trope', 'NN')]\n",
      "[]\n",
      "[]\n",
      "[('man-eating', 'NN')]\n",
      "[('piranhas', 'NN')]\n",
      "[('lends', 'NNS')]\n",
      "[('excitement', 'NN')]\n",
      "[]\n",
      "[]\n",
      "[('adventure', 'NN')]\n",
      "[('stories', 'NNS')]\n",
      "[]\n",
      "[('bears', 'NNS')]\n",
      "[('little', 'JJ')]\n",
      "[('resemblance', 'NN')]\n",
      "[]\n",
      "[]\n",
      "[('real-life', 'NN')]\n",
      "[('piranha', 'NN')]\n",
      "[]\n",
      "[('paying', 'VBG')]\n",
      "[]\n",
      "[('attention', 'NN')]\n",
      "[]\n",
      "[('fact', 'NN')]\n",
      "[]\n",
      "[('fiction', 'NN')]\n",
      "[('humans', 'NNS')]\n",
      "[('may', 'MD')]\n",
      "[('finally', 'RB')]\n",
      "[]\n",
      "[('able', 'JJ')]\n",
      "[]\n",
      "[('let', 'VB')]\n",
      "[('go', 'VB')]\n",
      "[]\n",
      "[]\n",
      "[('inaccurate', 'NN')]\n",
      "[('belief', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in token_list2:\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c31d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['despite', 'fact', 'piranhas', 'relatively', 'harmless', 'many', 'people', 'continue', 'believe', 'pervasive', 'myth', 'piranhas', 'dangerous', 'humans', 'impression', 'piranhas', 'exacerbated', 'mischaracterization', 'popular', 'media'] \n",
      "\n",
      "Total tokens :  67\n"
     ]
    }
   ],
   "source": [
    "token_list3 = list(filter(lambda token: token not in stopwords.words(\"english\"),token_list2))\n",
    "print(token_list3[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "314af107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d22e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['despite', 'fact', 'piranha', 'relatively', 'harmless', 'many', 'people', 'continue', 'believe', 'pervasive', 'myth', 'piranha', 'dangerous', 'human', 'impression', 'piranha', 'exacerbated', 'mischaracterization', 'popular', 'medium'] \n",
      "\n",
      "Total tokens :  67\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list4 = [lemmatizer.lemmatize(word) for word in token_list3]\n",
    "print(token_list4[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf2404c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b04995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list5 = []\n",
    "for i in token_list3:\n",
    "    token_list5.append(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab62ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['despit', 'fact', 'piranha', 'rel', 'harmless', 'mani', 'peopl', 'continu', 'believ', 'pervas', 'myth', 'piranha', 'danger', 'human', 'impress', 'piranha', 'exacerb', 'mischaracter', 'popular', 'media']\n"
     ]
    }
   ],
   "source": [
    "print(token_list5[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04c0cb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "despite : despit\n",
      "fact : fact\n",
      "piranhas : piranha\n",
      "relatively : rel\n",
      "harmless : harmless\n",
      "many : mani\n",
      "people : peopl\n",
      "continue : continu\n",
      "believe : believ\n",
      "pervasive : pervas\n",
      "myth : myth\n",
      "piranhas : piranha\n",
      "dangerous : danger\n",
      "humans : human\n",
      "impression : impress\n",
      "piranhas : piranha\n",
      "exacerbated : exacerb\n",
      "mischaracterization : mischaracter\n",
      "popular : popular\n",
      "media : media\n",
      "example : exampl\n",
      "promotional : promot\n",
      "poster : poster\n",
      "1978 : 1978\n",
      "horror : horror\n",
      "film : film\n",
      "piranha : piranha\n",
      "features : featur\n",
      "oversized : overs\n",
      "piranha : piranha\n",
      "poised : pois\n",
      "bite : bite\n",
      "leg : leg\n",
      "unsuspecting : unsuspect\n",
      "woman : woman\n",
      "terrifying : terrifi\n",
      "representation : represent\n",
      "easily : easili\n",
      "captures : captur\n",
      "imagination : imagin\n",
      "promotes : promot\n",
      "unnecessary : unnecessari\n",
      "fear : fear\n",
      "trope : trope\n",
      "man-eating : man-eat\n",
      "piranhas : piranha\n",
      "lends : lend\n",
      "excitement : excit\n",
      "adventure : adventur\n",
      "stories : stori\n",
      "bears : bear\n",
      "little : littl\n",
      "resemblance : resembl\n",
      "real-life : real-lif\n",
      "piranha : piranha\n",
      "paying : pay\n",
      "attention : attent\n",
      "fact : fact\n",
      "fiction : fiction\n",
      "humans : human\n",
      "may : may\n",
      "finally : final\n",
      "able : abl\n",
      "let : let\n",
      "go : go\n",
      "inaccurate : inaccur\n",
      "belief : belief\n"
     ]
    }
   ],
   "source": [
    "for i in token_list3:\n",
    "    print(i,\":\", ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aee35f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2d798e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idf_model  = TfidfVectorizer()\n",
    "tf_idf_vector = tr_idf_model.fit_transform(token_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69ff5175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'> (111, 81)\n"
     ]
    }
   ],
   "source": [
    "print(type(tf_idf_vector), tf_idf_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6e7ea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_array = tf_idf_vector.toarray()\n",
    "print(tf_idf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66abb16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1978', 'able', 'adventure', 'an', 'and', 'are', 'attention', 'be', 'bears', 'belief', 'believe', 'bite', 'by', 'captures', 'continue', 'dangerous', 'despite', 'easily', 'eating', 'exacerbated', 'example', 'excitement', 'fact', 'fear', 'features', 'fiction', 'film', 'finally', 'for', 'go', 'harmless', 'horror', 'humans', 'imagination', 'impression', 'in', 'inaccurate', 'is', 'it', 'leg', 'lends', 'let', 'life', 'little', 'man', 'many', 'may', 'media', 'mischaracterization', 'more', 'myth', 'of', 'oversized', 'paying', 'people', 'pervasive', 'piranha', 'piranhas', 'poised', 'popular', 'poster', 'promotes', 'promotional', 'real', 'relatively', 'representation', 'resemblance', 'stories', 'such', 'terrifying', 'than', 'that', 'the', 'their', 'this', 'to', 'trope', 'unnecessary', 'unsuspecting', 'while', 'woman']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "words_set = tr_idf_model.get_feature_names()\n",
    "print(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ae598fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1978</th>\n",
       "      <th>able</th>\n",
       "      <th>adventure</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>attention</th>\n",
       "      <th>be</th>\n",
       "      <th>bears</th>\n",
       "      <th>belief</th>\n",
       "      <th>...</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>their</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>trope</th>\n",
       "      <th>unnecessary</th>\n",
       "      <th>unsuspecting</th>\n",
       "      <th>while</th>\n",
       "      <th>woman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1978  able  adventure   an  and  are  attention   be  bears  belief  ...  \\\n",
       "0     0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     0.0  ...   \n",
       "1     0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     0.0  ...   \n",
       "2     0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     0.0  ...   \n",
       "3     0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     0.0  ...   \n",
       "4     0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     0.0  ...   \n",
       "..    ...   ...        ...  ...  ...  ...        ...  ...    ...     ...  ...   \n",
       "106   0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     0.0  ...   \n",
       "107   0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     0.0  ...   \n",
       "108   0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     0.0  ...   \n",
       "109   0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     0.0  ...   \n",
       "110   0.0   0.0        0.0  0.0  0.0  0.0        0.0  0.0    0.0     1.0  ...   \n",
       "\n",
       "     that  the  their  this   to  trope  unnecessary  unsuspecting  while  \\\n",
       "0     0.0  0.0    0.0   0.0  0.0    0.0          0.0           0.0    0.0   \n",
       "1     0.0  1.0    0.0   0.0  0.0    0.0          0.0           0.0    0.0   \n",
       "2     0.0  0.0    0.0   0.0  0.0    0.0          0.0           0.0    0.0   \n",
       "3     1.0  0.0    0.0   0.0  0.0    0.0          0.0           0.0    0.0   \n",
       "4     0.0  0.0    0.0   0.0  0.0    0.0          0.0           0.0    0.0   \n",
       "..    ...  ...    ...   ...  ...    ...          ...           ...    ...   \n",
       "106   0.0  0.0    0.0   0.0  0.0    0.0          0.0           0.0    0.0   \n",
       "107   0.0  0.0    0.0   0.0  0.0    0.0          0.0           0.0    0.0   \n",
       "108   0.0  0.0    0.0   1.0  0.0    0.0          0.0           0.0    0.0   \n",
       "109   0.0  0.0    0.0   0.0  0.0    0.0          0.0           0.0    0.0   \n",
       "110   0.0  0.0    0.0   0.0  0.0    0.0          0.0           0.0    0.0   \n",
       "\n",
       "     woman  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "..     ...  \n",
       "106    0.0  \n",
       "107    0.0  \n",
       "108    0.0  \n",
       "109    0.0  \n",
       "110    0.0  \n",
       "\n",
       "[111 rows x 81 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = words_set)\n",
    "df_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba550a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
